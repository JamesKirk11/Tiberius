{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccea2bc4",
   "metadata": {
    "id": "ccea2bc4"
   },
   "source": [
    "# Data Formating Overview\n",
    "\n",
    "**Contributors**: Lili Alderson, Munazza Alam, Natasha Batalha, Hannah Wakeford, add your name too if you'd like to contribute!\n",
    "\n",
    "This notebook was created to enable common formatting for the Early Release Science data analysis using `xarray` files. We will review: \n",
    "\n",
    "1. `xarray` 101\n",
    "2. Variable terminology\n",
    "3. File naming schemes \n",
    "4. Data formating \n",
    "5. Physical unit archiving \n",
    "\n",
    "This is namely for the booking of the following data products: \n",
    "\n",
    "1. Stellar spectra \n",
    "2. Raw light curves \n",
    "3. Fitted light curves \n",
    "4. Transit spectra\n",
    "\n",
    "### Packages to install before getting started \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3efc8d9",
   "metadata": {
    "id": "b3efc8d9",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Formating-Overview\" data-toc-modified-id=\"Data-Formating-Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Formating Overview</a></span></li><li><span><a href=\"#Data-Types:-Using-xarray\" data-toc-modified-id=\"Data-Types:-Using-xarray-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Types: Using <code>xarray</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#The-three-components-of-an-xarray-file\" data-toc-modified-id=\"The-three-components-of-an-xarray-file-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>The three components of an <code>xarray</code> file</a></span></li></ul></li><li><span><a href=\"#What-data-go-in-what-files?\" data-toc-modified-id=\"What-data-go-in-what-files?-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>What data go in what files?</a></span></li><li><span><a href=\"#Variable-Terminology-and-Required-data_vars,-coords-&amp;-attrs-for-each-file-type\" data-toc-modified-id=\"Variable-Terminology-and-Required-data_vars,-coords-&amp;-attrs-for-each-file-type-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Variable Terminology and Required <code>data_vars</code>, <code>coords</code> &amp; <code>attrs</code> for each file type</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stellar-Spectra\" data-toc-modified-id=\"Stellar-Spectra-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Stellar Spectra</a></span></li><li><span><a href=\"#Raw-Light-Curves\" data-toc-modified-id=\"Raw-Light-Curves-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Raw Light Curves</a></span></li><li><span><a href=\"#Fitted-Light-Curves\" data-toc-modified-id=\"Fitted-Light-Curves-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Fitted Light Curves</a></span></li><li><span><a href=\"#Transit-Spectra\" data-toc-modified-id=\"Transit-Spectra-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Transit Spectra</a></span></li></ul></li><li><span><a href=\"#Specifying-units\" data-toc-modified-id=\"Specifying-units-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Specifying units</a></span></li><li><span><a href=\"#xarray-Basics\" data-toc-modified-id=\"xarray-Basics-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><code>xarray</code> Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Easy-Example:-Fake-Transit-Spectra\" data-toc-modified-id=\"Easy-Example:-Fake-Transit-Spectra-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Easy Example: Fake Transit Spectra</a></span></li><li><span><a href=\"#2D-data:-e.g.-Raw-Light-Curves\" data-toc-modified-id=\"2D-data:-e.g.-Raw-Light-Curves-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>2D data: e.g. Raw Light Curves</a></span></li></ul></li><li><span><a href=\"#Storing-xarray-data\" data-toc-modified-id=\"Storing-xarray-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Storing <code>xarray</code> data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filenaming\" data-toc-modified-id=\"Filenaming-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Filenaming</a></span></li><li><span><a href=\"#Using-netcdf\" data-toc-modified-id=\"Using-netcdf-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Using <code>netcdf</code></a></span></li><li><span><a href=\"#Using-pickle\" data-toc-modified-id=\"Using-pickle-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Using <code>pickle</code></a></span></li></ul></li><li><span><a href=\"#Reading/interpreting-an-xarray-file\" data-toc-modified-id=\"Reading/interpreting-an-xarray-file-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Reading/interpreting an <code>xarray</code> file</a></span></li><li><span><a href=\"#Checking-your-data-is-in-compliance\" data-toc-modified-id=\"Checking-your-data-is-in-compliance-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Checking your data is in compliance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d86e4",
   "metadata": {
    "id": "b43d86e4"
   },
   "outputs": [],
   "source": [
    "!pip install netCDF4\n",
    "!pip install h5netcdf\n",
    "!pip install xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea7f08",
   "metadata": {
    "id": "1cea7f08"
   },
   "source": [
    "\n",
    "# Data Types: Using `xarray`\n",
    "\n",
    "[xarray: N-D labeled arrays and datasets in Python](https://docs.xarray.dev/en/stable/): From their website: \"array introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience. The package includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.\"\n",
    "\n",
    "Xarray is your friend and will make it very easy for other folks to use your data. \n",
    "\n",
    "## The three components of an `xarray` file\n",
    "\n",
    "### 1. `attrs` : attributes \n",
    "\n",
    "These are the \"attributes\" that contain any high level meta data. This could be things like author lists, parameters used in the data reduction, description of the code used, etc. \n",
    "\n",
    "### 2. `data_vars` : data variables \n",
    "\n",
    "These are the main \"data variables\". These are usually final results of your modeling or data reduction analysis. It will ultimately be what the user of your data will be looking for. \n",
    "\n",
    "### 3. `coords` : coordinate systems \n",
    "\n",
    "These are the coordinate system of your `data_vars`. Common coordinate systems be a wavelength grid for a flux array, or a time array for a time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb814c",
   "metadata": {
    "id": "bdbb814c"
   },
   "source": [
    "# What data go in what files? \n",
    "\n",
    "\n",
    "⭐️Prefer to see a clear break down of these in a [google sheet](https://docs.google.com/spreadsheets/d/1JZbKQJsu5YNpJLsgY-w21Sed9-D6uVPltMogB-ju0aY/edit#gid=0)??⭐️\n",
    "\n",
    "### File 1: Stellar Spectra \n",
    "\n",
    "Contains.. stellar spectra INSERT (could contain broader descriptions and/or link to codes that do these things or produce these products)\n",
    "\n",
    "**Suggested naming scheme** : `f'stellar-spec-planet{W39}-mode{G395H}-code{Eureka}-author{Alderson}.xc'` \n",
    "\n",
    "### File 2: Raw light curves\n",
    "\n",
    "Contains.. Raw light curves INSERT (could contain broader descriptions and/or link to codes that do these things or produce these products)\n",
    "\n",
    "**Suggested naming scheme** : `f'raw-light-curve-planet{W39}-mode{G395H}-code{Eureka}-author{Alderson}.xc'`  \n",
    "\n",
    "### File 3: Fitted light curves \n",
    "\n",
    "Contains... Fitted light curves INSERT (could contain broader descriptions and/or link to codes that do these things or produce these products)\n",
    "\n",
    "**Suggested naming scheme** : `f'fitted-light-curve-planet{W39}-mode{G395H}-code{Eureka}-author{Alderson}.xc'`\n",
    "\n",
    "### File 4: Transit Spectra \n",
    "\n",
    "Contains... Transit spectra (could contain broader descriptions and/or link to codes that do these things or produce these products)\n",
    "\n",
    "**Suggested naming scheme** : `f'transit-spectrum-planet{W39}-mode{G395H}-code{Eureka}-author{Alderson}.xc'` INSERT something you like \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abfb9a",
   "metadata": {
    "id": "a1abfb9a"
   },
   "source": [
    "# Variable Terminology and Required `data_vars`, `coords` & `attrs` for each file type\n",
    "\n",
    "⭐️Prefer to see a clear break down of these in a [google sheet](https://docs.google.com/spreadsheets/d/1JZbKQJsu5YNpJLsgY-w21Sed9-D6uVPltMogB-ju0aY/edit#gid=0)??⭐️\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0712f",
   "metadata": {
    "id": "44c0712f"
   },
   "source": [
    "## Stellar Spectra \n",
    "\n",
    "###  `attrs` \n",
    "\n",
    "#### Required `attrs`\n",
    "\n",
    "1. `author` (`str`): Author or author list \n",
    "2. `contact` (`str`): point of contact \n",
    "3. `code` (`str` or `dict`): code used. Sometimes people only use one code, in which case a string will suffice e.g., \"eureka\". Other times people might use a combination of many, in which case a dictionary can list the code used at each stage {\"Stage1\":eureka,\"Aperture\":my_pipeline_name,\"Stage2\":eureka} \n",
    "\n",
    "#### Optional  `attrs`\n",
    "1. `doi` (str): make sure to include if you want your work referenced!\n",
    "2. `extraction_params` (`dict`) : key parameters used to extract the 1D stellar spectra from the 2D integration images  `{'aperture_width_in_pixels': 6 'aperture_poly_order': 2}`\n",
    "3. `cleaning_params` (`dict`) : any relevant parameters or necessary information relating to how you cleaned the data, such as the [ramp  rejection threshold](https://jwst-pipeline.readthedocs.io/en/latest/jwst/jump/arguments.html)  e.g., `{'jump_threshold': 7, 'f_noise_method':'column by column'}`\n",
    "4. `notes` (str) : any additional reduction notes that you want the user to be aware of (e.g., you may wish to briefly explain the combination of codes used)\n",
    "\n",
    "### `data_vars`\n",
    "\n",
    "#### Requred `data_vars`\n",
    "\n",
    "1. `flux` (2D array) : array of 1D stellar spectra with shape (total number of stellar spectra, length of stellar spectrum) e.g., if your dataset contains 8000 integrations and the spectrum covers 4000 pixels this array would have shape (8000,4000) \n",
    "2. `flux_error` (2D array) : array of errors associated with the flux data. Must be same size as `flux` \n",
    "3. `quality_flag` (2D array) : a requirement of [chromatic](https://github.com/zkbt/chromatic), \"2D array indicating whether a particular flux data point is good (True) or bad (False)\". Must be same size as `flux`\n",
    "\n",
    "#### Optional  `data_vars`\n",
    "\n",
    "Optional `data_vars` here will include a variety of detrending parameters with potentially 1D (e.g. time) or 2D (e.g. time vs wavelength) dimensions. These products, which may be requested by light curve fitters, might include the pixel shifts of the spectral trace in the x and y directions, which would have lengths equal to the total number of stellar spectra. Below we provide an example set of `data_vars`. \n",
    "\n",
    "5. shift_x (1D array) :  pixel shift in x \n",
    "6. shift_y (1D array) : pixel shift in y \n",
    "7. others?? Zach?? \n",
    "\n",
    "### `coords`\n",
    "\n",
    "#### Requred `coords`\n",
    "\n",
    "1. `wavelength` (1D array) : wavelength array, should be same length as a stellar spectrum \n",
    "2. `time` (1D array) : time array, should be same length as the total number of stellar spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b007f20",
   "metadata": {
    "id": "8b007f20"
   },
   "source": [
    "\n",
    "## Raw Light Curves \n",
    "\n",
    "###  `attrs` \n",
    "\n",
    "#### Required `attrs`\n",
    "\n",
    "1. `author` (`str`): Author or author list \n",
    "2. `contact` (`str`): point of contact \n",
    "3. `code` (`str` or `dict`): code used. For example sometimes people only use one code \"eureka\", other times people might use many {\"Stage1\":jwst,\"Stage2\":my_pipeline}\n",
    "4. `data_origin` (`str`) : Who's stellar spectra did you use to make these? In order of preference: provide: 1) link to data doi, 2) link to data (e.g. personal drive), 3) link to paper doi, 4) contact email of author\n",
    "\n",
    "#### Optional  `attrs`\n",
    "1. `doi` (str): made sure to include if you want your work referenced! \n",
    "2. `notes` (str) : any additional reduction notes that you want the user to be aware of (e.g., you may wish to clarify if the contained light curves are binned spectrscopic light curves or broadband white light curves)\n",
    "\n",
    "### `data_vars` \n",
    "\n",
    "#### Required `data_vars` \n",
    "1. `raw_flux` (2D array) : array of 1D light curves with shape (total number of light curves, length of light curve) e.g., if your dataset contains 1000 binned light curves and each light curve contains 8000 time increments this array would have shape (1000,8000) \n",
    "2. `raw_flux_error` (2D array) : array of errors associated with the raw_flux data. Must be same size as raw_flux\n",
    "3. `quality_flag` (2D array) : a requirement of [chromatic](https://github.com/zkbt/chromatic), \"2D array indicating whether a particular flux data point is good (True) or bad (False)\" Must be same size as raw_flux\n",
    "\n",
    "#### Optional `data_vars` \n",
    "\n",
    "4. `detrending parameters` (`dict`) : any detrending parameters produced by your code that may be requested by light curve fitters. This might include the pixel shifts of the spectral trace in the x and y directions, e.g., `{'x_shifts': array_of_x, 'y_shifts': array_of_y}`\n",
    "\n",
    "### `coords` \n",
    "\n",
    "### Required `coords` \n",
    "\n",
    "1. `time_flux` (1D array) : time which light curves are plotted against, should be same length as number of raw_flux data points in each light curve (**Lili: note that in the stellar spec we called this `time`. here we are differentiating between `time_flux` so that in the next one there are the two different times. I am wondering if you want to change to `time_flux` in stellar spec to reduce time array names) **\n",
    "2. `central_wavelength` (1D array) : central wavelength associated with each `raw_flux` light curve, should be same length as total number of light curves\n",
    "\n",
    "### Optional `coords` \n",
    "\n",
    "3. `bin_half_width` (1D array) : half widths \n",
    "4. `start_wavelength` (1D array) : starting wavelength of each bin\n",
    "5. `end_wavelength` (1D array) : ending wavelength of each bin "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dd9ce",
   "metadata": {
    "id": "664dd9ce"
   },
   "source": [
    "## Fitted Light Curves\n",
    "\n",
    "###  `attrs` \n",
    "\n",
    "#### Required `attrs` \n",
    "\n",
    "1. `author` (`str`): Author or author list \n",
    "2. `contact` (`str`): point of contact \n",
    "3. `code` (`str` or `dict`): code used. For example sometimes people only use one code \"eureka\", other times people might use many {\"Stage1\":jwst,\"Stage2\":my_pipeline}\n",
    "4. `data_origin` (`dict`) : Who's stellar spectra and raw light curve did you use to make these? In order of preference: provide: 1) link to data doi, 2) link to data (e.g. personal drive), 3) link to paper doi, 4) contact email of author. For example: `{'stellar_spec':\"zenodo.org/xxxx\", 'raw_light_curve:\"zenodo.org/yyyy\"}`\n",
    "5. `system_params` (`dict`) : dictionary of relevant planet and system parameters. `{'rp': jupiter_radius, 'rs': stellar_radius , 'a_rs' : (a/Rs), 'tc' :  mid_transit_time}`\n",
    "6. `limb_darkening_params` (`dict`) : : dictionary of relevant limb darkening parameters.  `{'u1':x, 'u2':y,\t'u3':z,\t'u4':w}`\n",
    "\n",
    "#### Optional  `attrs`\n",
    "1. `doi` (str): made sure to include if you want your work referenced! \n",
    "2. `notes` (str) : any additional reduction notes that you want the user to be aware of (e.g., you may wish to clarify if the contained light curves are binned spectrscopic light curves or broadband white light curves) \n",
    "\n",
    "#### Required `data_vars` \n",
    "\n",
    "1. `raw_flux` (2D array) : array of 1D light curves before systematic detrending with shape (total number of light curves, length of light curve) e.g., if your dataset contains 1000 binned light curves and each light curve contains 8000 time increments this array would have shape (1000,8000) \n",
    "2. `raw_flux_error` (2D array) : array of errors associated with the raw_flux data. Must be same size as `raw_flux`\n",
    "3. `light_curve_model` (2D array) : array of 1D light curve models with shape (total number of light curve models, length of model). While the total number of light curves must be the same as for `raw_flux`, the length of the model may be different if the model is at a higher time resolution.\n",
    "4. `corrected_flux` (2D array) : array of 1D light curves after systematic detrending Must be same size as `raw_flux`\n",
    "5. `corrected_flux_error` (2D array) array of errors associated with the corrected_flux data. Must be same size as `raw_flux`\n",
    "6. `quality_flag` (2D array) : a requirement of chromatic, \"2D array indicating whether a particular flux data point is good (True) or bad (False)\" Must be same shape as `raw_flux`\n",
    "\n",
    "#### Optional `data_vars`\n",
    "\n",
    "7. `systematic_model` (2D array) : model used to detrend the light curves with shape (total number of light curve models, length of model). While the total number of light curves must be the same as for `raw_flux`, the length of the model may be different if the model is at a higher time resolution.\n",
    "8. `residuals` (2D array) : RMS residuals of the light curve fit. Must be same size as `raw_flux`\n",
    "\n",
    "Optional `data_vars` here will include a variety of detrending parameters with potentially 1D (e.g. time) or 2D (e.g. time vs wavelength) dimensions. These products, which may be requested by light curve fitters, might include the pixel shifts of the spectral trace in the x and y directions, which would have lengths equal to the total number of stellar spectra. Below we provide an example set of detrending parameters you might consider adding:\n",
    "\n",
    "9. shift_x (1D array) :  pixel shift in x \n",
    "10. shift_y (1D array) : pixel shift in y \n",
    "\n",
    "### `coords` \n",
    "\n",
    "#### Required `coords` \n",
    "\n",
    "1. `time_flux` (1D array) : time array for flux \n",
    "2. `time_model` (1D array) :\ttime array of systematic models \n",
    "3. `central_wavelength` (1D array): central wavelengths\n",
    "\n",
    "#### Optional `coords`\n",
    "\n",
    "4. `bin_half_width` (1D array) : half widths \n",
    "5. `start_wavelength` (1D array) : starting wavelength of each bin\n",
    "6. `end_wavelength` (1D array) : ending wavelength of each bin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab3bf9",
   "metadata": {
    "id": "94ab3bf9"
   },
   "source": [
    "## Transit Spectra\n",
    "\n",
    "###  `attrs` \n",
    "\n",
    "#### Required `attrs`\n",
    "\n",
    "\n",
    "1. `author` (`str`): Author or author list \n",
    "2. `contact` (`str`): point of contact \n",
    "3. `code` (`str` or `dict`): code used. For example sometimes people only use one code \"eureka\", other times people might use many {\"Stage1\":jwst,\"Stage2\":my_pipeline}\n",
    "4. `data_origin` (`dict`) : Who's stellar spectra, raw light curve, and fitted light curve did you use to make these? In order of preference: provide: 1) link to data doi, 2) link to data (e.g. personal drive), 3) link to paper doi, 4) contact email of author. For example: `{'stellar_spec':\"zenodo.org/xxxx\", 'raw_light_curve:\"zenodo.org/yyyy\", 'fitted_light_curve':\"zenodo.org/zzzz\"}`\n",
    "5. `system_params` (`dict`) : dictionary of relevant planet and system parameters. `{'rp': jupiter_radius, 'rs': stellar_radius , 'a_rs' : (a/Rs), 'tc' :  mid_transit_time}`\n",
    "6. `limb_darkening_params` (`dict`) : dictionary of relevant kimb darkening parameters.  `{'u1':x, 'u2':y,\t'u3':z,\t'u4':w}`\n",
    "\n",
    "#### Optional  `attrs`\n",
    "1. `doi` (str): made sure to include if you want your work referenced!  \n",
    "2. `notes` (str) : any additional information you want the user to be aware of\n",
    "\n",
    "###  `data_vars` \n",
    "\n",
    "#### Required `data_vars`\n",
    "\n",
    "1. `transit_depth` (required) : transit depth as (rp/rs)^2 (**Lili**: Note that this might become a larger part of a community framework.. you might think about making this \"optional\" along with \"eclipse_depth\", and/or phase curve \n",
    "2. `transit_depth_error` (required) : error on transit depth as 1-sigma (rp/rs)^2 \n",
    "\n",
    "### `coords` \n",
    "\n",
    "#### Required `coords` \n",
    "\n",
    "1. `central_wavelength` (1D array): central wavelengths\n",
    "\n",
    "#### Optional `coords`\n",
    "\n",
    "2. `bin_half_width` (1D array) : half widths \n",
    "3. `start_wavelength` (1D array) : starting wavelength of each bin\n",
    "4. `end_wavelength` (1D array) : ending wavelength of each bin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d84bb",
   "metadata": {
    "id": "479d84bb"
   },
   "source": [
    "\n",
    "# Specifying Physical Units\n",
    "\n",
    "Units are provided as Strings. However, we should be able to convert all units to `astropy.units`. \n",
    "\n",
    "For **physical unitless parameters** (e.g. transit_depth) unit should be provided such that they can still be astropy converted. E.g. area/area\n",
    "\n",
    "For **non-physical unitless parameters** (e.g. a bool) the unit field should be specified with a blank string. \n",
    "\n",
    "## Allowable non-physical units\n",
    "\n",
    "Allowable flux units that are not `astropy` friendly:\n",
    "\n",
    "- `e-`\n",
    "- `DN`\n",
    "\n",
    "\n",
    "## Specifying Time Units\n",
    "\n",
    "All time units should also use `astropy.time`. There is a [full tutorial available](https://docs.astropy.org/en/stable/time/index.html) through the astropy team. \n",
    "\n",
    "Example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99461aa9",
   "metadata": {
    "id": "99461aa9"
   },
   "outputs": [],
   "source": [
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ec110",
   "metadata": {
    "id": "463ec110"
   },
   "outputs": [],
   "source": [
    "#examples of valid units\n",
    "\n",
    "u.Unit('cm') #Valid \n",
    "#u.Unit('CM') #NOT valid\n",
    "u.Unit(\"R_jup\")#Valid\n",
    "u.Unit(\"R_jupiter\")#Valid\n",
    "#u.Unit(\"R_Jupiter\")#NOT Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731be94e",
   "metadata": {
    "id": "731be94e"
   },
   "outputs": [],
   "source": [
    "unit = 'cm'\n",
    "#doing it this away enables easy conversions. for example: \n",
    "(1*u.Unit('R_jup')).to('cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73c821",
   "metadata": {
    "id": "6c73c821"
   },
   "outputs": [],
   "source": [
    "u.Unit('micron')#valid\n",
    "u.Unit('um')#valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WAxyFkME0Tux",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "WAxyFkME0Tux",
    "outputId": "e1163c81-8b1f-484e-efb8-5bfda5f829f3"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\mathrm{}$"
      ],
      "text/plain": [
       "Unit(dimensionless)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transit depth \n",
    "u.Unit('(R_jup*R_jup)/(R_jup*R_jup)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cq6JuMPl0Y",
   "metadata": {
    "id": "40cq6JuMPl0Y"
   },
   "outputs": [],
   "source": [
    "from astropy.time import Time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WkUFjBTYSirz",
   "metadata": {
    "id": "WkUFjBTYSirz"
   },
   "outputs": [],
   "source": [
    "tm = Time(np.linspace(51544, 61544, 300), format='mjd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97986b39",
   "metadata": {
    "id": "97986b39"
   },
   "source": [
    "# `xarray` Basics\n",
    "\n",
    "[xarray: N-D labeled arrays and datasets in Python](https://docs.xarray.dev/en/stable/): From their website: \"array introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays, which allows for a more intuitive, more concise, and less error-prone developer experience. The package includes a large and growing library of domain-agnostic functions for advanced analytics and visualization with these data structures.\"\n",
    "\n",
    "Xarray is your friend and will make it very easy for other folks to use your data. Let's build some simple examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e94e1",
   "metadata": {
    "id": "058e94e1"
   },
   "source": [
    "## Easy Example: Fake Transit Spectra \n",
    "\n",
    "Here we will show an example where wavelength is the coordinate system, and transit depth is the final output product you are trying to share with the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a3f38",
   "metadata": {
    "id": "521a3f38"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import json #we will use this to dump model parameters into an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e66eb5",
   "metadata": {
    "id": "e4e66eb5"
   },
   "outputs": [],
   "source": [
    "#fake flat spectrum \n",
    "central_wavelength = np.linspace(1,10,500)\n",
    "bin_half_width = np.concatenate(([0.1],np.diff(np.linspace(1,10,500))))\n",
    "transit_depth_error = np.random.randn(500)+100e-6\n",
    "transit_depth = np.random.randn(500)*transit_depth_error+0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9488b52",
   "metadata": {
    "id": "f9488b52"
   },
   "source": [
    "Practice convert to `xarray`. In this case we are storing `transit_depth` data, labeled with `unitless` \"(rp/rs)^2\" that is on a grid of `central_wavelength` with units of \"micron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b145318",
   "metadata": {
    "id": "6b145318"
   },
   "outputs": [],
   "source": [
    "# put data into a dataset where each\n",
    "ds = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        transit_depth=([\"central_wavelength\"],\n",
    "                       transit_depth,{'units': 'unitless'}),#, required\n",
    "        transit_depth_error=([\"central_wavelength\"],\n",
    "                       transit_depth_error,{'units': 'unitless'}),#, required\n",
    "    ),\n",
    "    coords=dict(\n",
    "        central_wavelength=([\"central_wavelength\"], \n",
    "                             central_wavelength,{'units': 'micron'}),#required*\n",
    "        bin_half_width=([\"bin_half_width\"], \n",
    "                             bin_half_width,{'units': 'micron'})#required*\n",
    "    ),\n",
    "    attrs=dict(author=\"L Alderson\",#required\n",
    "               contact=\"lili.alderson13@gmail.com\",#required,\n",
    "               code=\"mycode\",#could also insert github link\n",
    "               data_origin=json.dumps({'stellar_spec':'zenodo.org/xxxx', \n",
    "                                   'raw_light_curve:'www.drive.google.com/someonegavemethis', \n",
    "                                   'fitted_light_curve':'malam@carnegiescience.edu'}), #required, in this case I used numpy to make my fake model. \n",
    "               doi=\"add your paper here\",#optional if there is a citation to reference\n",
    "               system_params=json.dumps({'rp':1, 'rs':1, 'a_rs':0.1, 'tc':0.9}), #optional in accordance with model runs\n",
    "               limb_darkening_params=json.dumps({'u1':1, 'u2':1,'u3':1, 'u4':1}), #optional in accordance with model runs\n",
    "              )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f88bb68",
   "metadata": {
    "id": "6f88bb68"
   },
   "outputs": [],
   "source": [
    "#printing is easy\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338cfd9",
   "metadata": {
    "id": "f338cfd9"
   },
   "outputs": [],
   "source": [
    "#plotting is easy\n",
    "ds['transit_depth'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ece2b",
   "metadata": {
    "id": "d21ece2b"
   },
   "source": [
    "## 2D data: e.g. Raw Light Curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678e42d",
   "metadata": {
    "id": "d678e42d"
   },
   "outputs": [],
   "source": [
    "time_flux = np.linspace(10,10000,400)#fake time array in BJD\n",
    "raw_flux = np.zeros((len(central_wavelength), len(time_flux))) + 1e8\n",
    "raw_flux_error = np.zeros((len(central_wavelength), len(time_flux))) + 1e8 \n",
    "quality_flag = np.zeros((len(central_wavelength), len(time_flux))) + 1e8 \n",
    "\n",
    "# put data into a dataset where each\n",
    "ds = xr.Dataset(\n",
    "    #now data is a function of two dimensions\n",
    "    data_vars=dict(raw_flux=([\"central_wavelength\",\"time_flux\"], raw_flux,{'units': 'ergs/(cm*cm*Angstrom)'}),\n",
    "                   raw_flux_error=([\"central_wavelength\",\"time_flux\"], raw_flux_error,{'units': 'ergs/(cm*cm*Angstrom)'}),\n",
    "                   quality_flag=([\"central_wavelength\",\"time_flux\"], quality_flag,{'units': 'unitless'}),\n",
    "                  ),\n",
    "    coords=dict(\n",
    "        central_wavelength=([\"central_wavelength\"], \n",
    "                             central_wavelength,{'units': 'micron'}),#required*\n",
    "        time_flux=([\"time_flux\"], \n",
    "                             time_flux,{'units': 'BJD'}),#required*\n",
    "        bin_half_width=([\"bin_half_width\"], \n",
    "                             bin_half_width,{'units': 'micron'})#required*\n",
    "    ),\n",
    "    attrs=dict(author=\"L Alderson\",#required\n",
    "               contact=\"lili.alderson13@gmail.com\",#required,\n",
    "               code=\"mycode\",#could also insert github link\n",
    "               data_origin=json.dumps({'stellar_spec':'zenodo.org/xxxx', #required, in this case I used numpy to make my fake model. \n",
    "               doi=\"add your paper here\",#optional if there is a citation to reference\n",
    "              )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e58d1",
   "metadata": {
    "id": "653e58d1"
   },
   "outputs": [],
   "source": [
    "#easy plotting \n",
    "ds['raw_flux'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e2d62",
   "metadata": {
    "id": "713e2d62"
   },
   "source": [
    "# Storing `xarray` data \n",
    "\n",
    "## Filenaming\n",
    "\n",
    "We usually rely on a long filename to give us information about the model. If we properly use `attrs` then filenaming does not matter. However, friendly filenames are always appreciated by people using your models. We suggest the following naming convention. \n",
    "\n",
    "Given independent variables (x,y,z): `tag_x_{x}_y_{y}_z_{z}.nc`\n",
    "\n",
    "For example: `stellar_spectra_target_w39_mode_G395H_code_eureka.nc`\n",
    "\n",
    "## Using `netcdf`\n",
    "\n",
    "\"The recommended way to store xarray data structures is netCDF, which is a binary file format for self-described datasets that originated in the geosciences. Xarray is based on the netCDF data model, so netCDF files on disk directly correspond to Dataset objects (more accurately, a group in a netCDF file directly corresponds to a Dataset object. See Groups for more.)\" - [Quoted from xarray website](https://docs.xarray.dev/en/stable/user-guide/io.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c4f4c",
   "metadata": {
    "id": "7e7c4f4c"
   },
   "outputs": [],
   "source": [
    "ds.to_netcdf(\"stellar_spectra_target_FAKE_mode_G395H_code_eureka.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be7de1",
   "metadata": {
    "id": "e9be7de1"
   },
   "source": [
    "## Using `pickle`\n",
    "\n",
    "Pickle is also an option, though not recommended because of differences in pickling between version of python. Additionally, netcdfs are more versatile across coding languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa90f5e5",
   "metadata": {
    "id": "fa90f5e5"
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "pk.dump(ds, open(\"stellar_spectra_target_FAKE_mode_G395H_code_eureka.pk\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd90a5",
   "metadata": {
    "id": "bfbd90a5"
   },
   "source": [
    "# Reading/interpreting an `xarray` file\n",
    "\n",
    "First, make sure you have installed [netCDF4](https://github.com/Unidata/netcdf4-python) and [h5netcdf](https://github.com/h5netcdf/h5netcdf) : \n",
    "\n",
    "```\n",
    "pip install netCDF4\n",
    "pip install h5netcdf\n",
    "```\n",
    "or if you prefer conda\n",
    "```\n",
    "conda install -c conda-forge netCDF4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94fbc0",
   "metadata": {
    "id": "1e94fbc0"
   },
   "outputs": [],
   "source": [
    "ds_ex = xr.open_dataset(\"stellar_spectra_target_FAKE_mode_G395H_code_eureka.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fc210",
   "metadata": {
    "id": "e34fc210"
   },
   "source": [
    "Look at all the information we can glean from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3e968",
   "metadata": {
    "id": "87c3e968"
   },
   "outputs": [],
   "source": [
    "ds_ex #39 data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d18b2",
   "metadata": {
    "id": "238d18b2"
   },
   "outputs": [],
   "source": [
    "ds_ex['central_wavelength']#data operates very similarly to pandas, note we can see the unit of the coordinate system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e4932",
   "metadata": {
    "id": "fd2e4932"
   },
   "outputs": [],
   "source": [
    "ds_ex['central_wavelength'].values #same as pandas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782cb8",
   "metadata": {
    "id": "b5782cb8"
   },
   "outputs": [],
   "source": [
    "ds_ex['raw_flux']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98a688",
   "metadata": {
    "id": "9e98a688"
   },
   "source": [
    "How to get attributes from string dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7aaf20",
   "metadata": {
    "id": "4a7aaf20"
   },
   "outputs": [],
   "source": [
    "json.loads(ds_sm.attrs['data_origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e0d9e",
   "metadata": {
    "id": "523e0d9e"
   },
   "outputs": [],
   "source": [
    "json.loads(ds_sm.attrs['data_origin'])['stellar_spectra'] #origin of stellar spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90746a4",
   "metadata": {
    "id": "f90746a4"
   },
   "source": [
    "# Checking your data is in compliance\n",
    "\n",
    "Coming soon upon finalization of data structions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0dfde",
   "metadata": {
    "id": "4ff0dfde"
   },
   "outputs": [],
   "source": [
    "def data_check(usr_xa):\n",
    "    \"\"\"This function will check that all the requirements have been met\"\"\"\n",
    "    \n",
    "    #step 1: check that required attributes are present \n",
    "    assert 'author' in usr_xa.attrs ,'No author information in attrs'\n",
    "    assert 'contact' in usr_xa.attrs ,'No contact information in attrs'\n",
    "    assert 'code' in usr_xa.attrs , 'Code used was not specified in attrs'\n",
    "    \n",
    "    #step 2: check that all coordinates have units\n",
    "    try: \n",
    "        for i in usr_xa.coords.keys(): test= usr_xa[i].units\n",
    "    except AttributeError: \n",
    "        print(f'Missing unit for {i} coords')\n",
    "\n",
    "    #step 2: check that all coordinates have units\n",
    "    try: \n",
    "        for i in usr_xa.data_vars.keys(): test=usr_xa[i].units\n",
    "    except AttributeError: \n",
    "        print(f'Missing unit for {i} data_var')\n",
    "    \"\"\"    #step 3: check that some attrs is a proper dictionary\n",
    "    try : \n",
    "        for i in usr_xa.attrs:\n",
    "            #these need to be dictionaries to be interpretable\n",
    "            if i in ['planet_params','stellar_params','cld_params','orbit_params']: \n",
    "                json.loads(usr_xa.attrs[i])\n",
    "    except ValueError: \n",
    "        print(f\"Was not able to read attr for {i}. This means that you did not properly define a dictionary with json and a dict.\",\" For example: json.dumps({'mp':1,'rp':1})\")\n",
    "    \n",
    "    #step 4: hurray if you have made it to here this is great\n",
    "    #last thing is the least important -- to make sure that we agree on terminology\n",
    "    for i in usr_xa.attrs: \n",
    "        if i == 'planet_params': \n",
    "            for model_key in json.loads(usr_xa.attrs[i]).keys():\n",
    "                assert model_key in ['rp', 'mp', 'tint', 'heat_redis', 'p_reference',\n",
    "                'pteff', 'mh' , 'cto' , 'logkzz'], f'Could not find {model_key} in listed planet_params attr. This might be because we havent added it yet! Check your terms and contact us if this is the case'\n",
    "        \n",
    "        elif  i == 'stellar_params': \n",
    "            for model_key in json.loads(usr_xa.attrs[i]).keys():\n",
    "                assert model_key in ['logg', 'feh', 'steff', 'rs', 'ms',\n",
    "                ], f'Could not find {model_key} in listed stellar_params attr. This might be because we havent added it yet! Check your terms and contact us if this is the case'\n",
    "        \n",
    "        elif  i == 'orbit_params': \n",
    "            for model_key in json.loads(usr_xa.attrs[i]).keys():\n",
    "                assert model_key in ['sma',\n",
    "                ], f'Could not find {model_key} in listed orbit_params attr. This might be because we havent added it yet! Check your terms and contact us if this is the case'\n",
    "        \n",
    "        elif  i == 'cld_params': \n",
    "            for model_key in json.loads(usr_xa.attrs[i]).keys():\n",
    "                assert model_key  in ['opd','ssa','asy','fsed'\n",
    "                ], f'Could not find {model_key} in listed cld_params attr. This might be because we havent added it yet! Check your terms and contact us if this is the case'\n",
    "    \"\"\"\n",
    "    \n",
    "    print('SUCCESS!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d4ad8",
   "metadata": {
    "id": "976d4ad8"
   },
   "outputs": [],
   "source": [
    "#ds_sm = xr.open_dataset(\"example_file.nc\")\n",
    "#data_check(ds_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea8dec",
   "metadata": {
    "id": "d3ea8dec"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Reduction Uniformity for JWST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
